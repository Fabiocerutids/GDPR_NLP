{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faceru\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from chat_with_assistant import GDPR_AI_Assistant\n",
    "from rag_qa import *\n",
    "from access_token import *\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "import time \n",
    "import random \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Created\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\faceru\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Pipeline Built\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\faceru\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "rag_assistant = GDPR_AI_Assistant()\n",
    "rag_assistant.create_session()\n",
    "system_prompt = \"\"\"Using the information contained in the context, give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question. \n",
    "Always mention the relevant GDPR articles.\n",
    "\"\"\"\n",
    "\n",
    "temperature=0.1\n",
    "max_length=100\n",
    "\n",
    "rag_assistant.set_system_parameters(prompt=system_prompt, temperature=temperature, max_length = max_length)\n",
    "\n",
    "system_prompt_general = (f\"\"\"{system_prompt}\"\"\"\n",
    "                    \"\\n\\n\"\n",
    "                    \"{context}\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", system_prompt_general),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "            )\n",
    "    \n",
    "general_llm = HuggingFaceEndpoint(\n",
    "        repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        task=\"text-generation\",\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        repetition_penalty=1.1,\n",
    "        return_full_text=False,\n",
    "        max_length=max_length,\n",
    "        huggingfacehub_api_token=my_huggingface_token\n",
    "        )\n",
    "general_llm_assistant = prompt | general_llm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXIN Privacy & Data Protection Foundation\n",
    "\n",
    "Source: https://dam.exin.com/api/&request=asset.permadownload&id=384&type=this&token=7ceee7e9b29ba9d2a28cc46fc3298e7d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [09:27<00:00, 14.18s/it]\n"
     ]
    }
   ],
   "source": [
    "rag_llm_answers = []\n",
    "general_llm_answers = []\n",
    "for q in tqdm(question_list):\n",
    "    q_ready = 'Please help me answer the following question:\\n'+q\n",
    "    rag_llm_answers.append(rag_assistant.chat_with_llm(q_ready, True))\n",
    "    time.sleep(1+random.random()*2)\n",
    "    general_llm_answers.append(general_llm_assistant.invoke({'context':'','input':q_ready}))\n",
    "    time.sleep(1+random.random()*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_clean_answers = [i[i.upper().find('ANSWER')+7:i.upper().find('ANSWER')+9].strip() for i in rag_llm_answers]\n",
    "general_clean_answers = [i[i.upper().find('ANSWER')+7:i.upper().find('ANSWER')+9].strip() for i in general_llm_answers]\n",
    "\n",
    "eval_df = pd.DataFrame({'Correct':question_answers, 'RAG':rag_clean_answers, 'General':general_clean_answers})\n",
    "#Fixing answers that were not extracted\n",
    "eval_df.loc[1,'General']='D'\n",
    "eval_df.loc[5,'General']='B'\n",
    "eval_df.loc[6,'General']='A'\n",
    "eval_df.loc[7,'General']='B'\n",
    "eval_df.loc[9,'General']='D'\n",
    "eval_df.loc[10,'General']='A'\n",
    "eval_df.loc[11,'General']='C'\n",
    "eval_df.loc[13,'RAG']='D'\n",
    "eval_df.loc[13,'General']='D'\n",
    "eval_df.loc[14,'RAG']='A'\n",
    "eval_df.loc[14,'General']='A'\n",
    "eval_df.loc[16,'General']='A'\n",
    "eval_df.loc[17,'RAG']='D'\n",
    "eval_df.loc[17,'General']='E'\n",
    "eval_df.loc[18,'General']='C'\n",
    "eval_df.loc[20,'RAG']='B'\n",
    "eval_df.loc[20,'General']='B'\n",
    "eval_df.loc[21,'General']='E'\n",
    "eval_df.loc[23,'General']='A'\n",
    "eval_df.loc[26,'General']='A'\n",
    "eval_df.loc[27,'General']='C'\n",
    "eval_df.loc[30,'General']='D'\n",
    "eval_df.loc[31,'General']='D'\n",
    "eval_df.loc[32,'General']='B'\n",
    "eval_df.loc[33,'General']='D'\n",
    "eval_df.loc[34,'RAG']='C'\n",
    "eval_df.loc[34,'General']='C'\n",
    "eval_df.loc[35,'General']='B'\n",
    "eval_df.loc[37,'General']='B'\n",
    "eval_df.loc[38,'General']='D'\n",
    "eval_df.loc[39,'General']='B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Accuracy: 0.625\n",
      "General Accuracy: 0.675\n"
     ]
    }
   ],
   "source": [
    "correct_rag = 0 \n",
    "correct_general = 0\n",
    "for i in range(eval_df.shape[0]):\n",
    "    if eval_df.loc[i, 'RAG']==eval_df.loc[i, 'Correct']:\n",
    "        correct_rag+=1\n",
    "    if eval_df.loc[i, 'General']==eval_df.loc[i, 'Correct']:\n",
    "        correct_general+=1\n",
    "print('RAG Accuracy:', correct_rag/eval_df.shape[0])\n",
    "print('General Accuracy:', correct_general/eval_df.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
